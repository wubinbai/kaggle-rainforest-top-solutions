

I trained simple classification models (24 binary classes) with logmel spectrograms :

    bootstrap stage: models are trained on TP/FP with masked BCE loss
    generate soft pseudo labels with 0.5 second sliding window
    train models with pseudo labels and also sample (with p=0.5) places with TP/FP - this partially solves confirmation bias problem.

Rounds of pseudo labeling and retraining (points 2,3) were repeated until the score on public LB didn't improve. Depending on the settings it took around 4-10 rounds to converge.

My initial models that gave 0.86 on TP/FP alone easily reached 0.96x with pseudo labeling . After this success I gave this challenge a 5 weeks break as I lost any motivation to improve my score :)
Later to my surprise it was extremely hard to beat 0.97 even with improved first stage models.
Melspectrogram parameters

    256 mel bins
    512 hop length
    original SR
    4096 nfft

FreqConv (CoordConv for frequency)

After my first successful experiment with pseudo labeling that reached 0.969 on public LB I tried to just swap encoders and blend models but this did not bring any improvements.
So I visualised the data for different classes and understood that when working with mel spectrograms for this task we don’t need translation invariance and classes really depend on both frequency and patterns.
I added a channel to CNN input which contains the number of mel bin scaled to 0-1. This significantly improved validation metrics and after this change log loss on crops around TP after the first round of training with pseudo labels was around 0.04 (same for crops around FP). Though it only slightly improved results on the LB.
First stage

For the first stage I used all tp/fp information without any sampling and made crops around the center of the signal.

Augmentations

    time warping
    random frequency masking below TP/FP signal
    random frequency masking above TP/FP signal
    gaussian noise
    volume gain
    mixup on spectrograms

For mixup on spectrograms - I used constant alpha (0.5) and hard labels with clipping (0,1). Masks were also added.
Pseudolabeling stages

Sampled TP/FP with p=0.5 otherwise made a random crop from the full spectrogram.

Without TP/FP sampling labels can become very soft and the score decreases after 2 or 3 rounds.
After training 4 folds of effnet/rexnet I generated OOF labels and ensembled their predictions. Then the training is repeated from scratch.

Augmentations

    gaussian noise
    volume gain
    mixup
    time warping
    spec augment
    mixup on spectrograms

Mixup

I used constant alpha (0.5) and added soft labels from two samples. This hurts logloss on FP a bit but at the same time significantly increases recall on TP.
Validation

Local validation did not have high correlation with the public leaderboard. Logloss on TP was somehow correlated but still it was not robust.
So without proper validation I decided to not select the best checkpoints and just trained 60 epochs (around 200 batches in each epoch) with CosineLR and AdamW optimizer.

My best models on validation - auc 0.999, log loss 0.03 did not produce great results (0.95). After the competition though It turned out that they can be easily improved with postprocessing to 97x-98x range.
Final ensemble

I used 4 models with 4 folds from Effnet and Rexnet (https://arxiv.org/abs/2007.00992 lightweight models with great performance) families:

    Rexnet-200 (4 sec training/inference), EffnetB3 (4 sec training/inference)
    Rexnet-150 (8 sec training/inference), EffnetB1 (8 sec training/inference)

Rexnet was much better than EfficientNet alone (less overfitting), but in ensemble they worked great.

During inference I just used 0.5 second sliding window and took max probabilities for the full clip and then averaged predictions from different models.
Lessons learned

I did not know about the paper and lacked this useful information about the dataset.

In my solutions I often rely on models alone but don’t explore the data deeply.
In this case I understood that the relabeled train set has similar class distribution to the test set and decided that models would easily learn that. I was wrong and simple post-processing could significantly improve results (though this happened due to severe class imbalance).
==================================
Did you try to add freqs to the loss function (it could force the network to use the FreqConv)?
I used CoorConv in some tasks and without adding the same coordinates to the loss function it didn't improve the network.
I have a simple check for it - pass 0 or noise to the channel with coordinates, when I didn't add coordinates to the loss function, it performs the same as with proper data in the CoordConv channel. When I trained with coordinates in the loss, 0 or noise in the CoordConv - network inferences much worse.
===============a =============
Good point! Just checked some checkpoint

With proper frequencies
neg_logloss: 0.1588028629548308
pos_logloss: 0.05171160377776966

With zeros
neg_logloss: 0.19649322897329777
pos_logloss: 0.3455986050609499

So my models really use it somehow
========== q ===========
Thanks for sharing your work. i am curious to learn new thing from your work
i have a question on masked BCE loss. how is this implemented. when you say mask, are you masking the loss for other classes that are not in the label or masking the time frames(say 4sec input where the label is only for sec1-2) where there is no label.

second question . how are your using both TP and FP. how this loss fn will look like.
===a ===
Masks for loss function have the same shape as labels, 1 for the classes we know (TP/FP) 0 for others

class BCEMasked(nn.Module):
   def forward(self, inputs, targets, mask=None):
       bce_loss = binary_cross_entropy_with_logits(inputs, targets, reduction='none')
       if mask is not None:
           bce_loss = bce_loss[mask > 0]
       return bce_loss.mean()
=========== q =========
           thanks for answering . about TP/FP , how are you using FP for training the model. with sigmoid you cant have FP = -1.
========  a ==========
example - FP for s0
mask=[1, 0, 0, …,0], targets=[0, 0, 0, …, 0] - only the first element from the targets and output will be considered by the loss function
======== q= ====
a question on this, do you train on the same random sampled audio crop and pseudo labels after or you re-sample tp/fp or made a random crop again in stage 2?
====  a ====
My pseudolabels (for 4 second models) had 113 frames per audio clip (0.5 sec sliding window).
If TP/FP is sampled - I made a random crop around the center and then found nearest frame from pseudolabels. Pseudo labels were fixed using tp/fp.
Otherwise I just took random frame from 113 and postprocessed labels if they overlap with tp/fp data.
=== q ==Rexnet-200 for 60 epochs.. wow) 
=== a == https://arxiv.org/abs/2007.00992 Rexnet-200 (200 = 2.0 scale, 150 = 1.5) is a lightweight model. It is not related to resnet, it is a modification of MobileNet that performs like EffNet.
So it was around 1 minute per epoch 
